{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_df = '../data/Beauty/ratings_Beauty.csv'\n",
    "df = pl.read_csv(\n",
    "    path_to_df, \n",
    "    has_header=False, \n",
    "    new_columns=['user_id', 'item_id', 'rating', 'timestamp'], \n",
    "    separator=',',\n",
    "    schema_overrides={\n",
    "        \"user_id\": pl.String,\n",
    "        \"item_id\": pl.String,\n",
    "        \"rating\": pl.String,\n",
    "        'timestamp': pl.UInt64\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_stage = 0\n",
    "is_changed = True\n",
    "threshold = 5\n",
    "good_users = set()\n",
    "good_items = set()\n",
    "\n",
    "filtered_df = df.clone()\n",
    "\n",
    "while is_changed:\n",
    "    user_counts = filtered_df.group_by(\"user_id\").agg(pl.len().alias(\"user_count\"))\n",
    "    item_counts = filtered_df.group_by(\"item_id\").agg(pl.len().alias(\"item_count\"))\n",
    "\n",
    "    good_users = user_counts.filter(pl.col(\"user_count\") >= threshold).select(\"user_id\")\n",
    "    good_items = item_counts.filter(pl.col(\"item_count\") >= threshold).select(\"item_id\")\n",
    "\n",
    "    old_size = len(filtered_df)\n",
    "\n",
    "    new_df = filtered_df.join(good_users, on=\"user_id\", how=\"inner\")\n",
    "    new_df = new_df.join(good_items, on=\"item_id\", how=\"inner\")\n",
    "\n",
    "    new_size = len(new_df)\n",
    "\n",
    "    print(f'После {filtering_stage + 1}го этапа фильтрации.')\n",
    "    print(f'Количество пользователей: {good_users.shape[0]}.') \n",
    "    print(f'Количество айтемов: {good_items.shape[0]}')\n",
    "    print()\n",
    "    \n",
    "    filtered_df = new_df\n",
    "    is_changed = old_size != new_size\n",
    "    filtering_stage += 1\n",
    "\n",
    "filtered_df = filtered_df.with_columns(new_user_id = pl.col(\"user_id\").rank(\"dense\") - 1)\n",
    "filtered_df = filtered_df.with_columns(new_item_id = pl.col(\"item_id\").rank(\"dense\") - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids_mapping = filtered_df.group_by('new_item_id').agg(pl.col('item_id')).select(\n",
    "    pl.col('item_id').list.get(0).alias('old_item_id'), pl.col('new_item_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102906dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.sort([\"new_user_id\", \"timestamp\"])\n",
    "\n",
    "grouped_filtered_df = filtered_df.group_by(\"new_user_id\", maintain_order=True).agg(\n",
    "    pl.all().exclude(\"new_user_id\").exclude('item_id').exclude('user_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3074f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e8605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Users count:', filtered_df.select('user_id').unique().shape[0])\n",
    "print('Items count:', filtered_df.select('item_id').unique().shape[0])\n",
    "print('Actions count:', filtered_df.shape[0])\n",
    "print('Avg user history len:', np.mean(list(map(lambda x: x[0], grouped_filtered_df.select(pl.col('new_item_id').list.len()).rows()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d3fbd3",
   "metadata": {},
   "source": [
    "## Content embedding creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f51ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            df[i] = eval(line)\n",
    "            i += 1\n",
    "\n",
    "    return pd.DataFrame.from_dict(df, orient=\"index\")\n",
    "\n",
    "df = getDF('../data/Beauty/metadata.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03506905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(row: pd.Series):\n",
    "    row = row.fillna(\"None\")\n",
    "    return f\"Title: {row['title']}. Categories: {', '.join(row['categories'][0])}. Description: {row['description']}.\"\n",
    "\n",
    "\n",
    "def get_data(metadata_df, item_ids_mapping_df):\n",
    "    filtered_df = metadata_df.join(\n",
    "        item_ids_mapping_df, \n",
    "        left_on=\"asin\", \n",
    "        right_on='old_item_id', \n",
    "        how=\"inner\"\n",
    "    ).select(pl.col('new_item_id'), pl.col('title'), pl.col('description'), pl.col('categories'))\n",
    "    print(filtered_df.shape)\n",
    "    print(filtered_df.head())\n",
    "\n",
    "    filtered_df = filtered_df.to_pandas()\n",
    "    filtered_df[\"combined_text\"] = filtered_df.apply(preprocess, axis=1)\n",
    "\n",
    "    import pickle\n",
    "    file = open('data.pkl', 'wb')\n",
    "    pickle.dump(filtered_df, file)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee63838",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(pl.from_pandas(df), item_ids_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a287a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "from transformers import LlamaModel, LlamaTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pickle\n",
    "\n",
    "file = open('data.pkl', 'rb')\n",
    "data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "model_name = \"huggyllama/llama-7b\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaModel.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "\n",
    "\n",
    "class MyDataset:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self._data = list(zip(data.to_dict()['new_item_id'].values(), data.to_dict()['combined_text'].values()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self._data[idx][1]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "        return {\n",
    "            'item_id': self._data[idx][0],\n",
    "            'input_ids': inputs['input_ids'][0],\n",
    "            'attention_mask': inputs['attention_mask'][0]\n",
    "        }\n",
    "    \n",
    "\n",
    "dataset = MyDataset(data)\n",
    "len(dataset)\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, drop_last=False, shuffle=False, num_workers=10)\n",
    "len(loader)\n",
    "\n",
    "\n",
    "new_df = {\n",
    "    'item_id': [],\n",
    "    'embedding': []\n",
    "}\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"].to(device), \n",
    "            attention_mask=batch[\"attention_mask\"].to(device)\n",
    "        )\n",
    "        embeddings = outputs.last_hidden_state\n",
    "    \n",
    "        embeddings = outputs.last_hidden_state  # (bs, sl, ed)\n",
    "        embeddings[(~batch[\"attention_mask\"].bool())] = 0. # (bs, sl, ed)\n",
    "\n",
    "    new_df['item_id'] += batch['item_id'].tolist()\n",
    "    new_df['embedding'] += embeddings.mean(dim=1).tolist()  # (bs, ed)\n",
    "\n",
    "\n",
    "file = open('final_data.pkl', 'wb')\n",
    "pickle.dump(new_df, file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82407f",
   "metadata": {},
   "source": [
    "## Leave-one-out split (last item for test, pre-last item for valid, the remaining part for train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5acd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = {}\n",
    "with open('../data/Beauty/all_data.txt', 'w') as f:\n",
    "    for user_id, _, _, item_ids in grouped_filtered_df.iter_rows():\n",
    "        json_data[str(user_id)] = item_ids\n",
    "        f.write(' '.join([str(user_id)] + [\n",
    "            str(item_id) for item_id in item_ids\n",
    "        ]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/Beauty/inter.json', 'w') as f:\n",
    "    json.dump(json_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e4fa0",
   "metadata": {},
   "source": [
    "## Timestamp-based split (80% for train, 10% for valid, and 10% for test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_portion = 0.1\n",
    "test_portion = 0.1\n",
    "\n",
    "all_events_timestamp = []\n",
    "for user_id, user_interractions in user_history.items():\n",
    "    for user_interraction in user_interractions:\n",
    "        interractions_ts = user_interraction['timestamp']\n",
    "        all_events_timestamp.append(interractions_ts)\n",
    "\n",
    "all_events_timestamp = sorted(all_events_timestamp)\n",
    "\n",
    "fst_threshold = all_events_timestamp[int(len(all_events_timestamp) * (1.0 - test_portion - valid_portion))]\n",
    "snd_threshold = all_events_timestamp[int(len(all_events_timestamp) * (1.0 - test_portion))]\n",
    "\n",
    "print(f'First train timestamp:\\t{all_events_timestamp[0]}')\n",
    "print(f'First valid timestamp:\\t{fst_threshold}')\n",
    "print(f'First test timestamp:\\t{snd_threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "valid_samples = []\n",
    "test_samples = []\n",
    "\n",
    "for user_id, user_interactions in user_history.items():\n",
    "    train_history = []\n",
    "    history = []\n",
    "    \n",
    "    for user_interaction in user_interactions:\n",
    "        if user_interaction['timestamp'] < fst_threshold: # train event\n",
    "            assert len(history) == 0 or user_interaction['timestamp'] >= history[-1]['timestamp']\n",
    "            train_history.append(user_interaction)\n",
    "        elif user_interaction['timestamp'] < snd_threshold: # valid event\n",
    "            assert user_interaction['timestamp'] >= fst_threshold\n",
    "            if len(history) >= 5:  # remove cold-start users\n",
    "                valid_samples.append({\n",
    "                    'user_id': user_id,\n",
    "                    'history': [x for x in history],\n",
    "                    'next_interaction': user_interaction\n",
    "                })\n",
    "        else:  # test event\n",
    "            assert user_interaction['timestamp'] >= snd_threshold\n",
    "            if len(history) >= 5:  # remove cold-start users\n",
    "                test_samples.append({\n",
    "                    'user_id': user_id,\n",
    "                    'history': [x for x in history],\n",
    "                    'next_interaction': user_interaction\n",
    "                })\n",
    "        history.append(user_interaction)\n",
    "    \n",
    "    if len(train_history) >= 5:  # remove cold-start users\n",
    "        train_samples.append({\n",
    "            'user_id': user_id,\n",
    "            'history': train_history\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f7ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_samples), len(valid_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e652e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "with open('../data/Beauty/train.txt', 'w') as f:\n",
    "    for train_sample in train_samples:\n",
    "        f.write(' '.join([str(train_sample['user_id'])] + [\n",
    "            str(user_interaction['item_id']) for user_interaction in sorted(train_sample['history'], key=lambda x: x['timestamp'])\n",
    "        ]))\n",
    "        f.write('\\n')\n",
    "\n",
    "# valid\n",
    "with open('../data/Beauty/valid.txt', 'w') as f:\n",
    "    for valid_sample in valid_samples:\n",
    "        f.write(' '.join([str(valid_sample['user_id'])] + [\n",
    "            str(user_interaction['item_id']) for user_interaction in sorted(valid_sample['history'], key=lambda x: x['timestamp'])\n",
    "        ] + [str(valid_sample['next_interaction']['item_id'])]))\n",
    "        f.write('\\n')\n",
    "\n",
    "# test\n",
    "with open('../data/Beauty/test.txt', 'w') as f:\n",
    "    for test_sample in test_samples:\n",
    "        f.write(' '.join([str(test_sample['user_id'])] + [\n",
    "            str(user_interaction['item_id']) for user_interaction in sorted(test_sample['history'], key=lambda x: x['timestamp'])\n",
    "        ] + [str(test_sample['next_interaction']['item_id'])]))\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
