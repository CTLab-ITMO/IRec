{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json, sys, os, gc, ast\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "project = 'AmazonReviews2018'\n",
        "path_to_raw = '../data/'+project+'/raw/'\n",
        "path_to_processed = '../data/'+project+'/'\n",
        "\n",
        "domains = {'Books':'Books.json',\n",
        "           'Movies':'Movies_and_TV.json'\n",
        "          }"
      ],
      "metadata": {
        "id": "9svb1E1J0Fm0"
      },
      "id": "9svb1E1J0Fm0",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is required for colab only\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path_to_raw = './drive/MyDrive/Science/Multidomain RecSys/' + path_to_raw[3:]\n",
        "path_to_processed = './drive/MyDrive/Science/Multidomain RecSys/source/iz-dev/' + path_to_processed[3:]"
      ],
      "metadata": {
        "id": "J-gtPgSxCtv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf4f0b4-06e6-4341-d6a3-3f0a63730e7c"
      },
      "id": "J-gtPgSxCtv1",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for d in tqdm(domains):\n",
        "    # part 1\n",
        "    new_dir = path_to_processed + d\n",
        "    if not os.path.isdir(new_dir):\n",
        "        os.makedirs(new_dir)\n",
        "\n",
        "    path_to_json = path_to_raw+d+'/'+domains[d]\n",
        "    path_to_tmp = new_dir+'/tmp_'+domains[d]\n",
        "    with open(path_to_tmp, 'w') as out_f:\n",
        "        pass\n",
        "\n",
        "    with open(path_to_json, 'r') as in_f, open(path_to_tmp, 'a') as out_f:\n",
        "        for line in in_f:\n",
        "            line = json.loads(line)\n",
        "            line = {\n",
        "                'user_id': line['reviewerID'],\n",
        "                'item_id': line['asin'],\n",
        "                'timestamp': line['unixReviewTime']\n",
        "            }\n",
        "            out_f.write(json.dumps(line)+'\\n')"
      ],
      "metadata": {
        "id": "0qDqTa_6jIkF"
      },
      "id": "0qDqTa_6jIkF",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in tqdm(domains):\n",
        "    new_dir = path_to_processed + d\n",
        "    path_to_json = new_dir +'/tmp_'+domains[d]\n",
        "\n",
        "    # part 2\n",
        "    print('\\n...part 2...')\n",
        "    user_interactions_count = {}\n",
        "    item_interactions_count = {}\n",
        "\n",
        "    with open(path_to_json, 'r') as f:\n",
        "        for line in f:\n",
        "            line = json.loads(line)\n",
        "            user_id = line['user_id']\n",
        "            item_id = line['item_id']\n",
        "            user_interactions_count[user_id] = user_interactions_count.get(user_id, 0) + 1\n",
        "            item_interactions_count[item_id] = item_interactions_count.get(item_id, 0) + 1\n",
        "\n",
        "    threshold = 5\n",
        "    good_users = set(user_id for user_id, count in user_interactions_count.items() if count >= threshold)\n",
        "    good_items = set(item_id for item_id, count in item_interactions_count.items() if count >= threshold)\n",
        "\n",
        "    del user_interactions_count, item_interactions_count\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    # part 3\n",
        "    print('...part 3...')\n",
        "    user_history = defaultdict(list)\n",
        "    item_history = defaultdict(list)\n",
        "\n",
        "    with open(path_to_json, 'r') as f:\n",
        "        for line in f:\n",
        "            line = json.loads(line)\n",
        "\n",
        "            user_raw_id = line['user_id']\n",
        "            item_raw_id = line['item_id']\n",
        "            interaction_timestamp = line['timestamp']\n",
        "\n",
        "            if user_raw_id in good_users and item_raw_id in good_items:\n",
        "                user_history[user_raw_id].append(item_raw_id)\n",
        "                item_history[item_raw_id].append(user_raw_id)\n",
        "\n",
        "    del good_users, good_items\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    # part 4\n",
        "    print('...part 4...')\n",
        "    is_changed = True\n",
        "    good_users = set()\n",
        "    good_items = set()\n",
        "\n",
        "    while is_changed:\n",
        "        old_state = (len(good_users), len(good_items))\n",
        "\n",
        "        good_users = set()\n",
        "        good_items = set()\n",
        "\n",
        "        for user_id, history in user_history.items():\n",
        "            if len(history) >= threshold:\n",
        "                good_users.add(user_id)\n",
        "\n",
        "        for item_id, history in item_history.items():\n",
        "            if len(history) >= threshold:\n",
        "                good_items.add(item_id)\n",
        "\n",
        "        user_history = {\n",
        "            user_id: list(filter(lambda x: x in good_items, history))\n",
        "            for user_id, history in user_history.items()\n",
        "        }\n",
        "\n",
        "        item_history = {\n",
        "            item_id: list(filter(lambda x: x in good_users, history))\n",
        "            for item_id, history in item_history.items()\n",
        "        }\n",
        "\n",
        "        new_state = (len(good_users), len(good_items))\n",
        "        is_changed = (old_state != new_state)\n",
        "        print(old_state, new_state)\n",
        "\n",
        "    del user_history, item_history, history\n",
        "    gc.collect()\n",
        "\n",
        "    with open(new_dir +'/tmp_good_users.txt', 'w') as f:\n",
        "        f.write(str(good_users))\n",
        "\n",
        "    with open(new_dir +'/tmp_good_items.txt', 'w') as f:\n",
        "        f.write(str(good_items))\n",
        "\n",
        "    del good_users, good_items\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "jAM_5ejozxHz"
      },
      "id": "jAM_5ejozxHz",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_counter = 0\n",
        "for d in tqdm(domains):\n",
        "    new_dir = path_to_processed + d\n",
        "    path_to_json = new_dir +'/tmp_'+domains[d]\n",
        "\n",
        "    # part 5\n",
        "    print('\\n...part 5...')\n",
        "    user_history = defaultdict(list)\n",
        "    item_history = defaultdict(list)\n",
        "    with open(new_dir +'/tmp_good_users.txt', 'r') as f:\n",
        "        good_users =  ast.literal_eval(f.read())\n",
        "    with open(new_dir +'/tmp_good_items.txt', 'r') as f:\n",
        "        good_items =  ast.literal_eval(f.read())\n",
        "\n",
        "    with open(path_to_json, 'r') as f:\n",
        "        for line in f:\n",
        "            line = json.loads(line)\n",
        "\n",
        "            user_raw_id = line['user_id']\n",
        "            item_raw_id = line['item_id']\n",
        "            interaction_timestamp = line['timestamp']\n",
        "\n",
        "            if user_raw_id in good_users and item_raw_id in good_items:\n",
        "                #user_history[user_raw_id].append({'item_id': item_raw_id, 'timestamp': interaction_timestamp})\n",
        "                #item_history[item_raw_id].append({'user_id': user_raw_id, 'timestamp': interaction_timestamp})\n",
        "                user_history[user_raw_id].append([item_raw_id, interaction_timestamp])\n",
        "                item_history[item_raw_id].append([user_raw_id, interaction_timestamp])\n",
        "\n",
        "            lines_counter += 1\n",
        "            if lines_counter % 1000000 == 0:\n",
        "                print('...%s lines processed...'%(str(lines_counter)))\n",
        "\n",
        "    with open(new_dir +'/tmp_user_history.json', 'a') as f:\n",
        "        for user_id,history in user_history.items():\n",
        "            f.write(json.dumps(str(user_id)+':'+str(history))+'\\n')\n",
        "\n",
        "    with open(new_dir +'/tmp_item_history.json', 'a') as f:\n",
        "        for item_id,history in item_history.items():\n",
        "            f.write(json.dumps(str(item_id)+':'+str(history))+'\\n')\n",
        "\n",
        "    del good_users, good_items\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "1mjLenKZssDc"
      },
      "id": "1mjLenKZssDc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in tqdm(domains):\n",
        "    new_dir = path_to_processed + d\n",
        "    path_to_json = new_dir +'/tmp_'+domains[d]\n",
        "\n",
        "    # part 6\n",
        "    print('...part 6...')\n",
        "    user_history = {}\n",
        "    item_history = {}\n",
        "\n",
        "    with open(new_dir +'/tmp_user_history.json', 'r') as f:\n",
        "        for line in f:\n",
        "            line = json.loads(line).split(':')\n",
        "            user_history[line[0]] = ast.literal_eval(line[1])\n",
        "\n",
        "    with open(new_dir +'/tmp_item_history.json', 'r') as f:\n",
        "        for line in f:\n",
        "            line = json.loads(line).split(':')\n",
        "            item_history[line[0]] = ast.literal_eval(line[1])\n",
        "\n",
        "    threshold = 5\n",
        "    user_mapping = {}\n",
        "    item_mapping = {}\n",
        "    tmp_user_history = defaultdict(list)\n",
        "    tmp_item_history = defaultdict(list)\n",
        "\n",
        "    for user_id, history in tqdm(user_history.items()):\n",
        "        processed_history = []\n",
        "\n",
        "        for filtered_item in history:\n",
        "            #item_id = filtered_item['item_id']\n",
        "            #item_timestamp = filtered_item['timestamp']\n",
        "            item_id = filtered_item[0]\n",
        "            item_timestamp = filtered_item[1]\n",
        "\n",
        "            processed_item_id = item_mapping.get(item_id, len(item_mapping) + 1)\n",
        "            item_mapping[item_id] = processed_item_id\n",
        "\n",
        "            #processed_history.append({'item_id': processed_item_id, 'timestamp': item_timestamp})\n",
        "            processed_history.append([processed_item_id, item_timestamp])\n",
        "\n",
        "        if len(processed_history) >= threshold:\n",
        "            processed_user_id = user_mapping.get(user_id, len(user_mapping) + 1)\n",
        "            user_mapping[user_id] = processed_user_id\n",
        "\n",
        "            #tmp_user_history[processed_user_id] = sorted(processed_history, key=lambda x: x['timestamp'])\n",
        "            tmp_user_history[processed_user_id] = sorted(processed_history, key=lambda x: x[1])\n",
        "\n",
        "\n",
        "    for item_id, history in tqdm(item_history.items()):\n",
        "        processed_history = []\n",
        "\n",
        "        for filtered_user in history:\n",
        "            #user_id = filtered_user['user_id']\n",
        "            #user_timestamp = filtered_user['timestamp']\n",
        "            user_id = filtered_user[0]\n",
        "            user_timestamp = filtered_user[1]\n",
        "\n",
        "            processed_user_id = user_mapping.get(user_id, len(user_mapping) + 1)\n",
        "            user_mapping[user_id] = processed_user_id\n",
        "\n",
        "            #processed_history.append({'user_id': processed_user_id, 'timestamp': user_timestamp})\n",
        "            processed_history.append([processed_user_id, user_timestamp])\n",
        "\n",
        "        if len(processed_history) >= threshold:\n",
        "            processed_item_id = item_mapping.get(item_id, len(item_mapping) + 1)\n",
        "            item_mapping[item_id] = processed_item_id\n",
        "\n",
        "            #tmp_item_history[processed_item_id] = sorted(processed_history, key=lambda x: x['timestamp'])\n",
        "            tmp_item_history[processed_item_id] = sorted(processed_history, key=lambda x: x[1])\n",
        "\n",
        "    user_history = tmp_user_history\n",
        "    item_history = tmp_item_history\n",
        "\n",
        "    del processed_history, history\n",
        "    del user_mapping, item_mapping\n",
        "    del tmp_user_history, tmp_item_history\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    # part 7\n",
        "    print('...part 7...')\n",
        "    for output_name in ['all_data.txt']:\n",
        "        with open(new_dir+'/'+output_name, 'w') as f:\n",
        "                for user_id, item_history in user_history.items():\n",
        "                    f.write(' '.join([str(user_id)] + [\n",
        "                        #str(item_event['item_id']) for item_event in sorted(item_history, key=lambda x: x['timestamp'])\n",
        "                        str(item_event[0]) for item_event in sorted(item_history, key=lambda x: x[1])\n",
        "                    ]))\n",
        "                    f.write('\\n')\n",
        "\n",
        "    del user_history, item_history\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "o4CTTdgQsv8e"
      },
      "id": "o4CTTdgQsv8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCUBRUAPZV2F"
      },
      "id": "sCUBRUAPZV2F",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}