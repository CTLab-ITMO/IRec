1) no biases on leave one out strategy (обрезаем по строго временному порогу)
2) data: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html#:~:text=image%20features-,Beauty,-reviews%20(2%2C023%2C070%20reviews



# На чем обучать? То есть на каких данных запускать backward pass?
# train model

# TODO fix collisisons (remainder = last embedding, auto-increment 4th id)

# 1 2 3 0
# 1 2 3 1
# 4 5 6 0/2
# 4 5 6 1/3

# Research last index aggregation

# 1) last index = KMeans(last residuals, n=|last codebook|) - collision
# 2) auto increment last index (check paper)
# 3) decoder
# 4) [(1 2 3), (1 2 3)] single item -> ok
# 4.1) several -> get embeddings -> score. softmax(collisions), torch.logsoftmax(logits) -> score -> argmax

# pos emb for item & codebook (000 111 222) - item
# codebook (012 012 012)
# splitting item ?

user_id & cb_ids -> repr
last 'seq' prediction
dataloader (semantic ids lens)